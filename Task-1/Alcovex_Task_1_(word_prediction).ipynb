{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "P.Gourav Pragvamsa"
      ],
      "metadata": {
        "id": "2aJhI1QXhhTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Prediction**"
      ],
      "metadata": {
        "id": "onUc8FCOhluZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKYPeVzahnNt"
      },
      "source": [
        "### Importing The Required Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8KQfkp6ZhnNu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3mKJ5SIQhnNv"
      },
      "outputs": [],
      "source": [
        "file = open(\"/content/dataset.txt\", \"r\", encoding = \"utf8\")\n",
        "lines = []\n",
        "\n",
        "for i in file:\n",
        "    lines.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnJk8enThnNx"
      },
      "source": [
        "### Cleaning the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Y6r4RN7MhnNx",
        "outputId": "62cad2dd-c9a6-4484-8cbd-92332e6daa90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data = \"\"\n",
        "\n",
        "for i in lines:\n",
        "    data = ' '. join(lines)\n",
        "    \n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data[:360]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "H1xXhccuhnNz",
        "outputId": "3a7d2aec-8a00-46ba-d494-372433f60e50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
        "new_data = data.translate(translator)\n",
        "\n",
        "new_data[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "4xVLps8dhnN0",
        "outputId": "bdb7ec06-b6c7-4c5b-eaa6-af4a874beb36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "z = []\n",
        "\n",
        "for i in data.split():\n",
        "    if i not in z:\n",
        "        z.append(i)\n",
        "        \n",
        "data = ' '.join(z)\n",
        "data[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqFvMYjXhnN1"
      },
      "source": [
        "### Tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDqDOB4ThnN1",
        "outputId": "b528bd0f-3412-4fa6-b685-5a430bad3e39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3b0nlVQhnN2",
        "outputId": "5d1b3f06-e9f8-4d41-8b81-6946256ad637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2617\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VL7cyiDohnN2",
        "outputId": "40f99429-86cf-49e7-e257-d8813ff69e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequences are:  3889\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 17,  53],\n",
              "       [ 53, 293],\n",
              "       [293,   2],\n",
              "       [  2,  18],\n",
              "       [ 18, 729],\n",
              "       [729, 135],\n",
              "       [135, 730],\n",
              "       [730, 294],\n",
              "       [294,   8],\n",
              "       [  8, 731]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kqEFbHXwhnN2"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0])\n",
        "    y.append(i[1])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDHZfIM_hnN3",
        "outputId": "be216178-920d-4d8a-cc57-852c68d3bd24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Data is:  [ 17  53 293   2  18]\n",
            "The responses are:  [ 53 293   2  18 729]\n"
          ]
        }
      ],
      "source": [
        "print(\"The Data is: \", X[:5])\n",
        "print(\"The responses are: \", y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD2hUr9EhnN3",
        "outputId": "ef78ec86-2046-423e-aec3-a84f4c7432f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7MLR3ZthnN3"
      },
      "source": [
        "### Creating the Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3LluL32fhnN3"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saduLIXUhnN4",
        "outputId": "c85f4a1d-1e8d-4faf-d38c-0cec37a7ca96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             26170     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2617)              2619617   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,694,787\n",
            "Trainable params: 15,694,787\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt3yPjDshnN4"
      },
      "source": [
        "### Plot The Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "4pJPSx3nhnN4",
        "outputId": "64e43f2e-d5f0-4ba0-e563-ebe41840d602"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAAIjCAYAAAAOft4aAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3deVRUZ54+8OcWFNSiVSAixCAoEBdQ42i7odguSSar3QoKKm49ZlwmY5tEJYm27eluE9Mu0J2WpI2OJzOdg8WSaOxt0pMYYqKmjSGaaNCo0cQfIkQRlEIo8Pv7w6Y6FV5kryqt53NOnSO33nrf77236vEuVfdqIiIgInKVq/N0BUTknRgORKTEcCAiJYYDESn5f3/CgQMHsHnzZk/UQkQekpub22haoy2Hb775Bnl5eW4piHxHXl4ezp8/7+ky6HvOnz/f5Oe90ZZDA1WSELWVpml48sknMX36dE+XQt+Rk5ODlJQU5XM85kBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJa8Oh+HDh8PPzw9Dhgzp0H4XLFiArl27QtM0fPrppy1u8+c//xlWqxV79uzp0HpaytPju8vBgwcxYMAA6HQ6aJqGsLAw/OpXv/J0WcjPz0d0dDQ0TYOmaQgPD0daWpqny+o0Xh0Ohw4dwoQJEzq8323btuHVV19tdRtPX8Xf0+O7y6hRo/DFF1/ggQceAACcOHECq1ev9nBVQFJSEs6cOYOYmBhYrVaUlJTgD3/4g6fL6jRNXuzFm2ia5ukSAACPPPIIKioqfHb86upqTJo0Cfv37/dYDe7ka/P7fV695dBAr9d3eJ8tCZzODCURQW5uLrZu3dppY3S07du3o7S01NNluI2vze/3dUg41NfXY82aNYiMjITRaMTgwYNhs9kAAJmZmTCbzdDpdBg2bBjCwsKg1+thNpsxdOhQJCYmolevXjAYDAgKCsLKlSsb9X/q1Cn0798fZrMZRqMRiYmJ+OCDD1o0PnDzg7hhwwb069cPgYGBsFqtWLFihcsYzbX54IMPEBkZCU3T8Lvf/Q4AkJWVBbPZDJPJhN27d+Ohhx6CxWJBREQEsrOzXep7/vnn0a9fPxiNRnTv3h19+vTB888/3+LLprV1/N/+9rcwGAzo0aMHFi1ahLvuugsGgwEJCQn46KOPAABLly5FQEAAwsPDneP9x3/8B8xmMzRNw7fffotly5bh6aefxunTp6FpGmJjY1tUd0e63eZ33759iIuLg9VqhcFgwKBBg/C///u/AG4e02o4dhETE4PCwkIAwPz582EymWC1WvHWW2/d8r3961//GiaTCV27dkVpaSmefvpp3H333Thx4kS7lrOTfI/NZhPF5Ftavny5BAYGSl5enpSXl8tzzz0nOp1ODh06JCIiP//5zwWAfPTRR1JVVSXffvutPPjggwJA/vSnP0lZWZlUVVXJ0qVLBYB8+umnzr4nTZok0dHR8tVXX4nD4ZDPP/9cRo4cKQaDQU6ePNmi8VetWiWapsmmTZukvLxc7Ha7bNmyRQBIYWFhi9t88803AkBeeuklZ32rVq0SAPLOO+9IRUWFlJaWSmJiopjNZqmtrRURkXXr1omfn5/s3r1b7Ha7HD58WMLCwmT8+PGtWs5tHX/hwoViNpvl+PHjcv36dTl27JgMHz5cunbtKl9//bWIiMyaNUvCwsJcxtuwYYMAkLKyMhERSUpKkpiYmFbV3ACA2Gy2Vr3mX//1XwWAlJeXe9X8xsTEiNVqbbb+3NxcWbt2rVy+fFkuXboko0aNkpCQEOfzSUlJ4ufnJ//v//0/l9fNnDlT3nrrLRFp2XsbgPz0pz+Vl156SaZOnSpffPFFs7U1uMXnPafdWw7Xr19HVlYWpkyZgqSkJAQFBWH16tXQ6/XYsWOHS9u4uDiYTCaEhIRgxowZAIDIyEh0794dJpPJeeS3qKjI5XVdu3ZF79694e/vj/j4eLz66qu4fv06tm7d2uz41dXVyMjIwH333YennnoKQUFBMBqN6Natm7P/lrRpTkJCAiwWC0JDQ5Gamoqqqip8/fXXAIBdu3Zh2LBhmDx5MoxGI4YOHYof/ehHeP/991FbW9um5d6a8QHA398fAwYMQGBgIOLi4pCVlYWrV682Wke3i9thfpOTk/Hzn/8cwcHB6NatGyZPnoxLly6hrKwMALB48WLU19e71FRZWYlDhw7h4YcfbtVna/369XjiiSeQn5+P/v37d0j97Q6HEydOwG63Y+DAgc5pRqMR4eHhjT7k3xUQEAAAqKurc05rOLbgcDhuOeagQYNgtVpx9OjRZsc/deoU7HY7Jk2a1GR/LWnTGg3z1jAf169fb3Smob6+Hnq9Hn5+fh0y5q3GV/nBD34Ak8l0y3V0u7hd5rfh/V1fXw8AmDhxIvr27Yv/+q//cr4/du7cidTUVPj5+bX5s9VR2h0OVVVVAIDVq1c796E0TcO5c+dgt9vbXWBT9Ho9HA5Hs+M33CshNDS0yb5a0qY9Hn74YRw+fBi7d+9GdXU1Pv74Y+zatQuPPvpop4RDSwUGBjr/F/MF7p7fP/3pTxg/fjxCQ0MRGBjY6HiapmlYtGgRzpw5g3feeQcA8N///d/4t3/7NwCe+2w1aHc4NHygMjIyICIujwMHDrS7QJW6ujpcvnwZkZGRzY5vMBgAADU1NU3215I27bF27VpMnDgR8+bNg8ViwdSpUzF9+vRmv2vRmRwOB65cuYKIiAiP1eBO7prf999/HxkZGfj6668xZcoUhIeH46OPPkJFRQVefPHFRu3nzZsHg8GAbdu24cSJE7BYLIiKigLgmc/Wd7X7ew4NZxqa+qZhZ9i7dy9u3LiBoUOHNjv+wIEDodPpUFBQgMWLF7e5TXscO3YMp0+fRllZGfz9veOrJe+99x5EBKNGjQJwcx+9ud2525m75vfw4cMwm8347LPP4HA4sGTJEkRHRwNQnxoPDg5GSkoKdu7cia5du+Lxxx93PueJz9Z3tXvLwWAwYP78+cjOzkZWVhYqKytRX1+P8+fP48KFCx1RI2pra1FRUYG6ujp88sknWLp0KaKiopype6vxQ0NDkZSUhLy8PGzfvh2VlZU4evSoy/cLWtKmPZ544glERkbi2rVrHdJfW9y4cQPl5eWoq6vD0aNHsWzZMkRGRmLevHkAgNjYWFy+fBm7du2Cw+FAWVkZzp0759JHt27dUFxcjLNnz+Lq1ateHSbunl+Hw4GLFy/ivffeg9lsRmRkJADg//7v/3D9+nV8+eWXzlOp37d48WLU1NTgj3/8Ix577DHndHd8tm6pFac2mlRTUyPp6ekSGRkp/v7+EhoaKklJSXLs2DHJzMwUk8kkAKR3796yb98+Wb9+vVitVgEgYWFh8vrrr8vOnTslLCxMAEhwcLBkZ2eLiMiOHTtkwoQJ0qNHD/H395eQkBCZMWOGnDt3rkXji4hcvXpVFixYICEhIdKlSxcZO3asrFmzRgBIRESEHDlypNk2jz/+uISHhwsAMZlMMnnyZNmyZYtz3u655x45ffq0bN26VSwWiwCQqKgoOXnypLz77rsSEhIiAJwPvV4vAwYMkPz8/BYt45deeqnN4y9cuFD0er3cfffd4u/vLxaLRX784x/L6dOnnf1funRJJkyYIAaDQfr06SP/+Z//KStWrBAAEhsbK19//bV88sknEhUVJUajUcaOHSslJSUtfo+gFacyDx48KPHx8aLT6QSAhIeHy7p16zw+vy+//LLExMS4rEfV44033hARkfT0dOnWrZsEBQXJtGnT5He/+50AkJiYGOcp1Qb/8i//Is8++2yjZXGr9/aLL74oRqNRAEivXr3kf/7nf1q8Phrc6lRmh4QD3dqWLVtk2bJlLtNqamrkySeflMDAQLHb7Z06/sKFC6Vbt26dOkZzWhMO7eUN89taDz/8sJw5c8bt494qHLxjB/gOVlJSgqVLlzbabwwICEBkZCQcDgccDgeMRmOn1tFw+sxXePv8OhwO56nNo0ePwmAwoE+fPh6uytVt8duK25nRaIRer8f27dtx8eJFOBwOFBcXY9u2bVizZg2GDBkCq9XqcqpK9UhNTfX0rFAHSk9Px5dffomTJ09i/vz5+OUvf+npkhphOHQyq9WKt99+G59//jn69u0Lo9GIuLg47NixA+vXr8dHH33U6DSV6rFz5842jf/cc89hx44dqKioQJ8+fZCXl9fBc+hdbpf5NZlM6N+/P+677z6sXbsWcXFxni6pEU3E9at7OTk5SElJ8ZlrB5B7aJoGm83W4h+akXvc4vOeyy0HIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAipSYv9jJt2jR31kE+ICMjA7m5uZ4ug76j4bYMKo1+sn3gwAFs3ry504si71JWVoYvvvgC48aN83Qp5AGK0M5tFA7km3gdD/oeXs+BiNQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlPw9XQC53/nz5zF37lzU19c7p3377bfw9/fH+PHjXdr269cPv//9791cIXkDhoMPioiIwNmzZ3HmzJlGzxUUFLj8nZiY6K6yyMtwt8JHzZkzB3q9vtl2qampbqiGvBHDwUfNmjULDofjlm3i4uIQHx/vporI2zAcfFRsbCwGDx4MTdOUz+v1esydO9fNVZE3YTj4sDlz5sDPz0/5XF1dHaZPn+7misibMBx82IwZM3Djxo1G0zVNw8iRI9G7d2/3F0Veg+Hgw3r27ImEhATodK5vAz8/P8yZM8dDVZG3YDj4uNmzZzeaJiJISkryQDXkTRgOPm7atGkuWw5+fn6477770KNHDw9WRd6A4eDjgoOD8cADDzgPTIoI0tLSPFwVeQOGAyEtLc15YNLf3x+TJ0/2cEXkDRgOhMmTJyMwMND5b4vF4uGKyBu47bcVOTk57hqK2mDo0KHYv38/+vTpw3XlxXr16oXRo0e7ZSxNRMQtAzXxTTwiarnk5GTk5ua6Y6hct+5W2Gw2iAgfXvSw2WwAgNraWqxcudLj9fDR9CM5OdmdH1cec6Cb9Ho91q5d6+kyyIswHMjJaDR6ugTyIgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTkc+EwfPhw+Pn5YciQIR3a74IFC9C1a1domoZPP/20xW3+/Oc/w2q1Ys+ePR1aT2fJz89HdHQ0NE1r8tER97vgevI8nwuHQ4cOYcKECR3e77Zt2/Dqq6+2uo2IW66102GSkpJw5swZxMTEwGq1Oq81UFdXB7vdjosXL8JkMrV7HK4nz3PbZeK8jbdcmeqRRx5BRUWFp8toNz8/PxiNRhiNRvTt27fD+uV68hyf23Jo0JLbz7dWS97InflmFxHk5uZi69atnTZGS+zatavD+uJ68hyvDYf6+nqsWbMGkZGRMBqNGDx4sPOSZpmZmTCbzdDpdBg2bBjCwsKg1+thNpsxdOhQJCYmolevXjAYDAgKCsLKlSsb9X/q1Cn0798fZrMZRqMRiYmJ+OCDD1o0PnBzBW/YsAH9+vVDYGAgrFYrVqxY4TJGc20++OADREZGQtM0/O53vwMAZGVlwWw2w2QyYffu3XjooYdgsVgQERGB7Oxsl/qef/559OvXD0ajEd27d0efPn3w/PPPe80NcLmebo/11CRxEwBis9la3H758uUSGBgoeXl5Ul5eLs8995zodDo5dOiQiIj8/Oc/FwDy0UcfSVVVlXz77bfy4IMPCgD505/+JGVlZVJVVSVLly4VAPLpp586+540aZJER0fLV199JQ6HQz7//HMZOXKkGAwGOXnyZIvGX7VqlWiaJps2bZLy8nKx2+2yZcsWASCFhYUtbvPNN98IAHnppZec9a1atUoAyDvvvCMVFRVSWloqiYmJYjabpba2VkRE1q1bJ35+frJ7926x2+1y+PBhCQsLk/Hjx7dqvdhsNmnL2yAmJkasVqvLtJ/+9Kfy2WefuUzjeuqY9SQikpycLMnJya1+XRvleGU4VFdXi8lkktTUVOc0u90ugYGBsmTJEhH555vu6tWrzjavvfaaAHB5g/79738XALJz507ntEmTJsm9997rMubRo0cFgCxfvrzZ8e12u5hMJrn//vtd+sjOzna+oVrSRuTWb7rq6mrntIY366lTp0REZPjw4TJixAiXvv/93/9ddDqd1NTU3GrxumhPOABo9GgqHLie/qkt60nE/eHglbsVJ06cgN1ux8CBA53TjEYjwsPDUVRU1OTrAgICAAB1dXXOaQ37rA6H45ZjDho0CFarFUePHm12/FOnTsFut2PSpElN9teSNq3RMG8N83H9+vVGR9Dr6+uh1+udt7brbN89WyEi+OlPf9qi13E9uXc9tZVXhkNVVRUAYPXq1S7nz8+dOwe73d5p4+r1ejgcjmbHP3/+PAAgNDS0yb5a0qY9Hn74YRw+fBi7d+9GdXU1Pv74Y+zatQuPPvqox950mZmZLh/UzsL15B5eGQ4NKyojI6PRtfsPHDjQKWPW1dXh8uXLiIyMbHZ8g8EAAKipqWmyv5a0aY+1a9di4sSJmDdvHiwWC6ZOnYrp06c3ew7/dsf15D5eGQ4NR7Cb+gZbZ9i7dy9u3LiBoUOHNjv+wIEDodPpUFBQ0GR/LWnTHseOHcPp06dRVlYGh8OBr7/+GllZWQgODu6U8VrjwoULmD9/fqf0zfXkPl4ZDgaDAfPnz0d2djaysrJQWVmJ+vp6nD9/HhcuXOiQMWpra1FRUYG6ujp88sknWLp0KaKiojBv3rxmxw8NDUVSUhLy8vKwfft2VFZW4ujRoy7nrVvSpj2eeOIJREZG4tq1ax3SX0cQEVRXVyM/P7/DbsbL9eRB7jr0iVaeyqypqZH09HSJjIwUf39/CQ0NlaSkJDl27JhkZmaKyWQSANK7d2/Zt2+frF+/XqxWqwCQsLAwef3112Xnzp0SFhYmACQ4OFiys7NFRGTHjh0yYcIE6dGjh/j7+0tISIjMmDFDzp0716LxRUSuXr0qCxYskJCQEOnSpYuMHTtW1qxZIwAkIiJCjhw50mybxx9/XMLDwwWAmEwmmTx5smzZssU5b/fcc4+cPn1atm7dKhaLRQBIVFSUnDx5Ut59910JCQlxOVOg1+tlwIABkp+f3+Ll3NqzFW+88UaTZyq++1i9ejXXUweuJxGeyqQW2rJliyxbtsxlWk1NjTz55JMSGBgodru9Rf209VQmtUxHrScR94eDz/624nZWUlKCpUuXNtrXDggIQGRkJBwOBxwOB29v52G3+3ryymMOdGtGoxF6vR7bt2/HxYsX4XA4UFxcjG3btmHNmjVITU3tsH1+arvbfT0xHG5DVqsVb7/9Nj7//HP07dsXRqMRcXFx2LFjB9avX4/XXnvN0yUSbv/1xN2K21RiYiL+9re/eboMasbtvJ645UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTk1l9ldtaVo6ntGtZJTk6Ohyuh5pw/fx4RERFuG08Tcc+9xb3lbslEt7Pk5GTk5ua6Y6hct205uCmDqI1ycnKQkpLC9UROPOZAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESv6eLoDcr6ysDG+++abLtI8//hgAsHXrVpfpXbp0wcyZM91WG3kPTUTE00WQe9XU1CA0NBRVVVXw8/MDAIgIRAQ63T83Jh0OB+bMmYPXXnvNU6WS5+Ryt8IHBQYGYtq0afD394fD4YDD4UBdXR3q6+udfzscDgDgVoMPYzj4qJkzZ6K2tvaWbYKCgjBp0iQ3VUTehuHgoyZMmIDQ0NAmn9fr9UhLS4O/Pw9L+SqGg4/S6XSYOXMmAgIClM87HA7MmDHDzVWRN2E4+LAZM2Y0uWtx1113YfTo0W6uiLwJw8GHjRw5ElFRUY2m6/V6zJ07F5qmeaAq8hYMBx83e/Zs6PV6l2ncpSCA4eDzZs2a5Txt2SA2NhaDBw/2UEXkLRgOPq5///6Ii4tz7kLo9XrMnz/fw1WRN2A4EObMmeP8pqTD4cD06dM9XBF5A4YDITU1FfX19QCAYcOGITY21sMVkTdgOBCioqIwfPhwADe3IogA/vCqEZ6+803JycnIzc31dBneJJffjVVYtmzZHfcFoAMHDiAzMxM2m035fGVlJbKysvDMM8+4uTLPy8jI8HQJXonhoDB69Og78qBcZmbmLefrhz/8Ie655x43VuQduMWgxmMO5OSLwUBNYzgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRzaYePGjejRowc0TcMrr7zi6XI6VH5+PqKjo6FpGjRNQ3h4ONLS0m75miNHjiA1NRV9+vRBYGAgunfvjnvvvRe/+tWvANy8HF1Df8095s+f7zL+z372s1uOvXnzZmiaBp1Oh/79++P999/vsGXhqxgO7bB8+XLs37/f02V0iqSkJJw5cwYxMTGwWq0oKSnBH/7whybbf/bZZ0hISEB4eDj27t2LiooK7N+/Hw8++CDee+89Z7u3334bV65cgcPhwIULFwAAkydPRm1tLaqqqlBaWorHH3/cZXwA2LZtW6NL6Deor6/Hb3/7WwDAxIkTUVRUhHHjxnXQkvBdDAc3q66uRkJCgqfL6HAbN25EUFAQMjMz0bt3bxgMBvTt2xe//OUvYTQaAdy8BN+YMWNgtVpdbtCraRr0ej1MJhNCQ0MxbNgwl76HDRuGkpIS7Nq1Szl2fn4+7r777s6bOR/FcHCz7du3o7S01NNldLhLly6hoqICly9fdpkeEBCAPXv2AACys7NhMpma7WvhwoV49NFHnX8vWbIEAPDyyy8r22/evBlPP/10W0unJjAcOkFBQQFGjBgBk8kEi8WCQYMGobKyEsuWLcPTTz+N06dPQ9M0xMbGIjMzE2azGTqdDsOGDUNYWBj0ej3MZjOGDh2KxMRE9OrVCwaDAUFBQVi5cqWnZ09p+PDhqKqqwsSJE/Hhhx92aN8TJ07EgAEDsHfvXpw4ccLluQ8//BB2ux0PPPBAh45JDIcOV1VVhcmTJyM5ORmXL1/Gl19+ib59+6K2thaZmZl47LHHEBMTAxHBqVOnsGzZMqxYsQIigpdffhlfffUVSkpKMG7cOBQWFuLZZ59FYWEhLl++jLlz52LDhg04cuSIp2ezkZUrV+IHP/gBjhw5grFjxyI+Ph6//vWvG21JtNWiRYsAoNGB302bNuGpp57qkDHIFcOhg509exaVlZWIj4+HwWBAWFgY8vPz0b1792ZfGxcXB5PJhJCQEOeNbCMjI9G9e3eYTCbn2YKioqJOnYe2MBqN2L9/P37zm9+gf//+OH78ONLT0zFgwAAUFBS0u/+5c+fCbDbjtddeQ3V1NQDgzJkzOHToEGbOnNnu/qkxhkMHi46ORo8ePZCWloa1a9fi7NmzbeonICAAAFBXV+ec1nA37KaO2nuaXq/H0qVL8cUXX+DgwYP48Y9/jNLSUkybNg3l5eXt6ttqtWLmzJkoLy/Hzp07Ady8pPySJUucy4o6FsOhgxmNRrz77rsYO3Ys1q1bh+joaKSmpjr/t/MVI0eOxJtvvonFixejrKwMe/fubXefDQcmX3nlFVy5cgW5ubnO3Q3qeAyHThAfH489e/aguLgY6enpsNls2Lhxo6fL6nDvv/++84YwSUlJLls5DWbPng0AsNvt7R5vyJAhGDVqFP7+979j4cKFmDZtGoKDg9vdL6kxHDpYcXExjh8/DgAIDQ3FCy+8gKFDhzqn3UkOHz4Ms9kMAKipqVHOY8PZhcGDB3fImA1bD3l5eXjyySc7pE9SYzh0sOLiYixatAhFRUWora1FYWEhzp07h1GjRgEAunXrhuLiYpw9exZXr1712uMHt+JwOHDx4kW89957znAAgClTpiAnJwdXrlxBRUUFdu/ejWeeeQY/+tGPOiwcpk+fju7du2PKlCmIjo7ukD6pCUIuAIjNZmtR202bNklYWJgAELPZLFOnTpWzZ89KQkKCBAcHi5+fn/Ts2VNWrVoldXV1IiLyySefSFRUlBiNRhk7dqw8++yzYjKZBID07t1b9u3bJ+vXrxer1SoAJCwsTF5//XXZuXOnc6zg4GDJzs5u1XzZbDZpzep+4403JCYmRgDc8vHGG2+IiMjbb78tKSkpEhMTI4GBgRIQECD9+vWTtWvXyvXr1136rqyslHHjxkm3bt0EgOh0OomNjZV169Ypx+/evbs88cQTzudWrlwp+/fvd/69evVqCQ8Pd/YVFxcn+/bta/G8JicnS3Jycovb+4gc3mX7ezRNg81mu+PulZmTk4OUlBRwdTc2bdo0ALxn5vfkcreCiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUeCWo79E0zdMlkAckJyfzSlCucv2bb+NbbDabp0vwiAMHDiAzM9Nn579Xr16eLsHrcMuBAPAak9QIryFJRGoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJbimhU8AABkQSURBVIYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEjJ39MFkPs5HA5cu3bNZVpVVRUAoLy83GW6pmkICgpyW23kPRgOPujSpUuIiIhAfX19o+e6devm8vf48eOxd+9ed5VGXoS7FT4oPDwc48aNg05369WvaRpmzJjhpqrI2zAcfNTs2bOhadot2+h0OiQlJbmpIvI2DAcflZSUBD8/vyaf9/Pzw4MPPoiQkBA3VkXehOHgoywWCx588EH4+6sPO4kI0tLS3FwVeROGgw9LS0tTHpQEgICAADz66KNuroi8CcPBhz322GMwmUyNpvv7+2PKlCno0qWLB6oib8Fw8GEGgwFTp06FXq93mV5XV4dZs2Z5qCryFgwHHzdz5kw4HA6XaRaLBffff7+HKiJvwXDwcffdd5/LF5/0ej1SU1MREBDgwarIGzAcfJy/vz9SU1OduxYOhwMzZ870cFXkDRgOhBkzZjh3LcLCwpCYmOjhisgbMBwIY8aMQc+ePQHc/OZkc1+rJt/gUz+82rx5Mw4cOODpMrxS165dAQCFhYWYNm2ah6vxTk899RRGjx7t6TLcxqf+izhw4AAOHjzo6TK8UmRkJPz9/XHixAlPl+KV8vLy8M0333i6DLfyqS0HABg1ahRyc3M9XYZXGj16NCIiIrh8FJr7kdqdyKe2HOjWIiIiPF0CeRGGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLD4RY2btyIHj16QNM0vPLKK54up1k3btxARkYGEhIS3DJefn4+oqOjoWkaNE1DeHh4s3fJOnLkCFJTU9GnTx8EBgaie/fuuPfee/GrX/0KAJCamursr7nH/PnzXcb/2c9+dsuxN2/eDE3ToNPp0L9/f7z//vsdtizuRAyHW1i+fDn279/v6TJa5Msvv8S4cePw1FNPwW63u2XMpKQknDlzBjExMbBarSgpKcEf/vCHJtt/9tlnSEhIQHh4OPbu3YuKigrs378fDz74IN577z1nu7fffhtXrlyBw+HAhQsXAACTJ09GbW0tqqqqUFpaiscff9xlfADYtm1bo8vsN6ivr8dvf/tbAMDEiRNRVFSEcePGddCSuDMxHDpYdXW12/7nbnDkyBE888wzWLx4MYYMGeLWsVtj48aNCAoKQmZmJnr37g2DwYC+ffvil7/8JYxGI4CbF1UZM2YMrFary308NU2DXq+HyWRCaGgohg0b5tL3sGHDUFJSgl27dinHzs/Px9133915M3cHYjh0sO3bt6O0tNStY957773Iz8/HrFmzEBgY6NaxW+PSpUuoqKjA5cuXXaYHBARgz549AIDs7GzlLfq+b+HChS738lyyZAkA4OWXX1a237x5M55++um2lu6TGA5tUFBQgBEjRsBkMsFisWDQoEGorKzEsmXL8PTTT+P06dPQNA2xsbHIzMyE2WyGTqfDsGHDEBYWBr1eD7PZjKFDhyIxMRG9evWCwWBAUFAQVq5c6enZ6zTDhw9HVVUVJk6ciA8//LBD+544cSIGDBiAvXv3NroO5ocffgi73Y4HHnigQ8e80zEcWqmqqgqTJ09GcnIyLl++jC+//BJ9+/ZFbW0tMjMz8dhjjyEmJgYiglOnTmHZsmVYsWIFRAQvv/wyvvrqK5SUlGDcuHEoLCzEs88+i8LCQly+fBlz587Fhg0bcOTIEU/PZqdYuXIlfvCDH+DIkSMYO3Ys4uPj8etf/7rRlkRbLVq0CAAaHTzetGkTnnrqqQ4Zw5cwHFrp7NmzqKysRHx8PAwGA8LCwpCfn4/u3bs3+9q4uDiYTCaEhIRgxowZAG5e9bl79+4wmUzOI/1FRUWdOg+eYjQasX//fvzmN79B//79cfz4caSnp2PAgAEoKChod/9z586F2WzGa6+9hurqagDAmTNncOjQId7Fqw0YDq0UHR2NHj16IC0tDWvXrsXZs2fb1E/DvSjr6uqc0757S7o7lV6vx9KlS/HFF1/g4MGD+PGPf4zS0lJMmzYN5eXl7erbarVi5syZKC8vx86dOwEAGRkZWLJkCe/92QYMh1YyGo149913MXbsWKxbtw7R0dFITU11/k9FLTdy5Ei8+eabWLx4McrKyrB3795299lwYPKVV17BlStXkJub69zdoNZhOLRBfHw89uzZg+LiYqSnp8Nms2Hjxo2eLssrvf/++8jIyABw83sR391SajB79mwA6JDvZwwZMgSjRo3C3//+dyxcuBDTpk1DcHBwu/v1RQyHViouLsbx48cBAKGhoXjhhRcwdOhQ5zRydfjwYZjNZgBATU2Ncjk1nF0YPHhwh4zZsPWQl5eHJ598skP69EUMh1YqLi7GokWLUFRUhNraWhQWFuLcuXMYNWoUAKBbt24oLi7G2bNncfXq1Tv6+MGtOBwOXLx4Ee+9954zHABgypQpyMnJwZUrV1BRUYHdu3fjmWeewY9+9KMOC4fp06eje/fumDJlCqKjozukT58kPiQ5OVmSk5Nb3H7Tpk0SFhYmAMRsNsvUqVPl7NmzkpCQIMHBweLn5yc9e/aUVatWSV1dnYiIfPLJJxIVFSVGo1HGjh0rzz77rJhMJgEgvXv3ln379sn69evFarUKAAkLC5PXX39ddu7c6RwrODhYsrOzW1zngQMHZMyYMXLXXXcJAAEg4eHhkpCQIAUFBZ22fN544w2JiYlxjtnU44033hARkbfffltSUlIkJiZGAgMDJSAgQPr16ydr166V69evu/RdWVkp48aNk27dugkA0el0EhsbK+vWrVOO3717d3niiSecz61cuVL279/v/Hv16tUSHh7u7CsuLk727dvX4nkFIDabrcXt7wA5moiIuwPJUxruHs17Qapx+TRN0zTYbDZMnz7d06W4Sy53K4hIieHgpYqKilr0s+XU1FRPl0p3KP/mm5An9O/fHz60x0deiFsORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlLyuZ9sHzx40HnFI3J18OBBAODyIQA+Fg6jR4/2dAleq6ysDLW1tbwtfROSk5PRq1cvT5fhVj51DUlqWk5ODlJSUniBGWrAa0gSkRrDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCRkr+nCyD3O3/+PObOnYv6+nrntG+//Rb+/v4YP368S9t+/frh97//vZsrJG/AcPBBEREROHv2LM6cOdPouYKCApe/ExMT3VUWeRnuVvioOXPmQK/XN9suNTXVDdWQN2I4+KhZs2bB4XDcsk1cXBzi4+PdVBF5G4aDj4qNjcXgwYOhaZryeb1ej7lz57q5KvImDAcfNmfOHPj5+Smfq6urw/Tp091cEXkThoMPmzFjBm7cuNFouqZpGDlyJHr37u3+oshrMBx8WM+ePZGQkACdzvVt4Ofnhzlz5nioKvIWDAcfN3v27EbTRARJSUkeqIa8CcPBx02bNs1ly8HPzw/33XcfevTo4cGqyBswHHxccHAwHnjgAeeBSRFBWlqah6sib8BwIKSlpTkPTPr7+2Py5Mkeroi8AcOBMHnyZAQGBjr/bbFYPFwReQP+tuIfcnJyPF2CRw0dOhT79+9Hnz59fHpZ9OrVC6NHj/Z0GV5BExHxdBHeoKlvCpJvSU5ORm5urqfL8Aa53K34DpvNBhHxqYfNZgMA1NbWYuXKlR6vx5OP5ORkD78DvQvDgQDc/C3F2rVrPV0GeRGGAzkZjUZPl0BehOFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDh1kwYIF6Nq1KzRNw6effurpcjpNfn4+oqOjoWmayyMgIAA9evTA+PHjsWHDBpSXl3u6VGonhkMH2bZtG1599VVPl9HpkpKScObMGcTExMBqtUJEcOPGDZSWliInJwd9+vRBeno64uPj8fHHH3u6XGoHhgO1m6ZpCAoKwvjx47Fjxw7k5OTg4sWLeOSRR1BRUeHp8qiNGA4diJeauyk5ORnz5s1DaWkpXnnlFU+XQ23EcGgjEcGGDRvQr18/BAYGwmq1YsWKFS5t6uvrsWbNGkRGRsJoNGLw4MHOy7JlZWXBbDbDZDJh9+7deOihh2CxWBAREYHs7GxnHwUFBRgxYgRMJhMsFgsGDRqEysrKZvv3tHnz5gEA/vKXvwDw7WVx2xISEREAYrPZWtx+1apVommabNq0ScrLy8Vut8uWLVsEgBQWFoqIyPLlyyUwMFDy8vKkvLxcnnvuOdHpdHLo0CFnHwDknXfekYqKCiktLZXExEQxm81SW1sr165dE4vFIi+++KJUV1dLSUmJTJ06VcrKylrUf0vYbDZpy9sgJiZGrFZrk89XVlYKAOnVq9dtsyySk5MlOTm51cviDpXDcPiH1oSD3W4Xk8kk999/v8v07OxsZzhUV1eLyWSS1NRUl9cFBgbKkiVLROSfH4jq6mpnm4aAOXXqlHz++ecCQP74xz82qqEl/bdEZ4WDiIimaRIUFHTbLAuGg4sc7la0walTp2C32zFp0qQm25w4cQJ2ux0DBw50TjMajQgPD0dRUVGTrwsICAAAOBwOREdHo0ePHkhLS8PatWtx9uzZdvfvLlVVVRARWCwWn18WtyuGQxucP38eABAaGtpkm6qqKgDA6tWrXb4PcO7cOdjt9haNYzQa8e6772Ls2LFYt24doqOjkZqaiurq6g7pvzOdPHkSANC/f3+fXxa3K4ZDGxgMBgBATU1Nk20agiMjI6PR/REOHDjQ4rHi4+OxZ88eFBcXIz09HTabDRs3buyw/jvLX//6VwDAQw895PPL4nbFcGiDgQMHQqfToaCgoMk2vXr1gsFgaNe3JYuLi3H8+HEAN8PmhRdewNChQ3H8+PEO6b+zlJSUICMjAxEREfjJT37i08vidsZwaIPQ0FAkJSUhLy8P27dvR2VlJY4ePYqtW7c62xgMBsyfPx/Z2dnIyspCZWUl6uvrcf78eVy4cKFF4xQXF2PRokUoKipCbW0tCgsLce7cOYwaNapD+m8vEcG1a9dw48YNiAjKyspgs9kwZswY+Pn5YdeuXbBYLD6xLO5Ibj4C6rXQylOZV69elQULFkhISIh06dJFxo4dK2vWrBEAEhERIUeOHJGamhpJT0+XyMhI8ff3l9DQUElKSpJjx47Jli1bxGQyCQC555575PTp07J161axWCwCQKKiouRvf/ubJCQkSHBwsPj5+UnPnj1l1apVUldXJyJyy/5bqrVnK9566y0ZPHiwmEwmCQgIEJ1OJwCcZyZGjBghv/jFL+TSpUsur7sdlgXPVrjI4Y10/0HTNNhsNkyfPt3TpbhVTk4OUlJSwLcBMG3aNADgjXRv4o10iUiN4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIyd/TBXgTX7xSccM85+TkeLgSzzt//jwiIiI8XYbX4GXi/oE3wSXg5k2AeZk4AEAutxz+wdczkteSpO/jMQciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJS8vd0AeR+ZWVlePPNN12mffzxxwCArVu3ukzv0qULZs6c6bbayHtoIiKeLoLcq6amBqGhoaiqqoKfnx8AQEQgItDp/rkx6XA4MGfOHLz22mueKpU8J5e7FT4oMDAQ06ZNg7+/PxwOBxwOB+rq6lBfX+/82+FwAAC3GnwYw8FHzZw5E7W1tbdsExQUhEmTJrmpIvI2DAcfNWHCBISGhjb5vF6vR1paGvz9eVjKVzEcfJROp8PMmTMREBCgfN7hcGDGjBluroq8CcPBh82YMaPJXYu77roLo0ePdnNF5E0YDj5s5MiRiIqKajRdr9dj7ty50DTNA1WRt2A4+LjZs2dDr9e7TOMuBQEMB583a9Ys52nLBrGxsRg8eLCHKiJvwXDwcf3790dcXJxzF0Kv12P+/Pkeroq8AcOBMGfOHOc3JR0OB6ZPn+7hisgbMBwIqampqK+vBwAMGzYMsbGxHq6IvAHDgRAVFYXhw4cDuLkVQQT4wA+vcnJykJKS4uky6A5zh39sACDXZ74ba7PZPF2CV6usrERWVhaeeeYZ5fMpKSlYtmyZz38x6sCBA8jMzPR0GW7hM+HAg2zN++EPf4h77rlH+VxKSgpGjx7N5Qj4TDjwmAM5NRUM5JsYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMhxZYsGABunbtCk3T8Omnn3q6nFa7ceMGMjIykJCQ4Jbx8vPzER0dDU3TXB4BAQHo0aMHxo8fjw0bNqC8vNwt9VDbMBxaYNu2bXj11Vc9XUabfPnllxg3bhyeeuop2O12t4yZlJSEM2fOICYmBlarFSKCGzduoLS0FDk5OejTpw/S09MRHx+Pjz/+2C01UesxHO5gR44cwTPPPIPFixdjyJAhHq1F0zQEBQVh/Pjx2LFjB3JycnDx4kU88sgjqKio8GhtpMZwaKHb8dZw9957L/Lz8zFr1iwEBgZ6uhwXycnJmDdvHkpLS/HKK694uhxSYDgoiAg2bNiAfv36ITAwEFarFStWrHBpU19fjzVr1iAyMhJGoxGDBw92XqcyKysLZrMZJpMJu3fvxkMPPQSLxYKIiAhkZ2c7+ygoKMCIESNgMplgsVgwaNAgVFZWNtv/nWLevHkAgL/85S8AuEy9jtzhbDabtHY2V61aJZqmyaZNm6S8vFzsdrts2bJFAEhhYaGIiCxfvlwCAwMlLy9PysvL5bnnnhOdTieHDh1y9gFA3nnnHamoqJDS0lJJTEwUs9kstbW1cu3aNbFYLPLiiy9KdXW1lJSUyNSpU6WsrKxF/bfWyJEj5d57723Ta0VEAIjNZmvVa2JiYsRqtTb5fGVlpQCQXr16icjtsUzb8n66TeXc8XPZ2pVpt9vFZDLJ/fff7zI9OzvbGQ7V1dViMpkkNTXV5XWBgYGyZMkSEfnnG7m6utrZpiFgTp06JZ9//rkAkD/+8Y+NamhJ/63ljeEgIqJpmgQFBd02y9SXwoG7Fd9z6tQp2O12TJo0qck2J06cgN1ux8CBA53TjEYjwsPDUVRU1OTrAgICANy85Vx0dDR69OiBtLQ0rF27FmfPnm13/7ebqqoqiAgsFguXqRdiOHzP+fPnAQChoaFNtqmqqgIArF692uU8/rlz51p8utBoNOLdd9/F2LFjsW7dOkRHRyM1NRXV1dUd0v/t4OTJkwBu3syXy9T7MBy+x2AwAABqamqabNMQHBkZGRARl8eBAwdaPFZ8fDz27NmD4uJipKenw2azYePGjR3Wv7f761//CgB46KGHuEy9EMPhewYOHAidToeCgoIm2/Tq1QsGg6Fd35YsLi7G8ePHAdwMmxdeeAFDhw7F8ePHO6R/b1dSUoKMjAxERETgJz/5CZepF2I4fE9oaCiSkpKQl5eH7du3o7KyEkePHsXWrVudbQwGA+bPn4/s7GxkZWWhsrIS9fX1OH/+PC5cuNCicYqLi7Fo0SIUFRWhtrYWhYWFOHfuHEaNGtUh/XsLEcG1a9dw48YNiAjKyspgs9kwZswY+Pn5YdeuXbBYLFym3sjNR0Ddri1Hl69evSoLFiyQkJAQ6dKli4wdO1bWrFkjACQiIkKOHDkiNTU1kp6eLpGRkeLv7y+hoaGSlJQkx44dky1btojJZBIAcs8998jp06dl69atYrFYBIBERUXJ3/72N0lISJDg4GDx8/OTnj17yqpVq6Surk5E5Jb9t9SBAwdkzJgxctdddwkAASDh4eGSkJAgBQUFrVomaMXZirfeeksGDx4sJpNJAgICRKfTCQDnmYkRI0bIL37xC7l06ZLL626HZepLZyt85i7bd/hsdjpN02Cz2Xz+Xpk+9H7K5W4FESkxHG4zRUVFjX4KrXqkpqZ6ulS6zfl7ugBqnf79+/vCJi15AW45EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEjJZ36yfTve69LbpKSkICUlxdNlkJvc8eGQkJDA+yEStcEdfw1JImoTXkOSiNQYDkSkxHAgIiV/ALmeLoKIvM7B/w+AR+ipsSHsewAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmAARMYrhnN4"
      },
      "source": [
        "### Callbacks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6-h3nOEuhnN4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto')\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "\n",
        "logdir='logsnextword1'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDfKvgNlhnN5"
      },
      "source": [
        "### Compile The Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiE8RS17hnN5",
        "outputId": "eca8ff1a-5757-43dc-c03e-9719340adeed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFmETUxyhnN5"
      },
      "source": [
        "### Fit The Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI0__KtvhnN5",
        "outputId": "4f04fa45-64f7-4db5-a4bc-e342753d51b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8755\n",
            "Epoch 1: loss improved from inf to 7.87553, saving model to nextword1.h5\n",
            "61/61 [==============================] - 27s 345ms/step - loss: 7.8755 - lr: 0.0010\n",
            "Epoch 2/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8618\n",
            "Epoch 2: loss improved from 7.87553 to 7.86181, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 324ms/step - loss: 7.8618 - lr: 0.0010\n",
            "Epoch 3/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8251\n",
            "Epoch 3: loss improved from 7.86181 to 7.82510, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 330ms/step - loss: 7.8251 - lr: 0.0010\n",
            "Epoch 4/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.6879\n",
            "Epoch 4: loss improved from 7.82510 to 7.68787, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 331ms/step - loss: 7.6879 - lr: 0.0010\n",
            "Epoch 5/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.4943\n",
            "Epoch 5: loss improved from 7.68787 to 7.49427, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 329ms/step - loss: 7.4943 - lr: 0.0010\n",
            "Epoch 6/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.3069\n",
            "Epoch 6: loss improved from 7.49427 to 7.30689, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 314ms/step - loss: 7.3069 - lr: 0.0010\n",
            "Epoch 7/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.1792\n",
            "Epoch 7: loss improved from 7.30689 to 7.17923, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 338ms/step - loss: 7.1792 - lr: 0.0010\n",
            "Epoch 8/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.0323\n",
            "Epoch 8: loss improved from 7.17923 to 7.03235, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 335ms/step - loss: 7.0323 - lr: 0.0010\n",
            "Epoch 9/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.7977\n",
            "Epoch 9: loss improved from 7.03235 to 6.79765, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 319ms/step - loss: 6.7977 - lr: 0.0010\n",
            "Epoch 10/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.5296\n",
            "Epoch 10: loss improved from 6.79765 to 6.52959, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 344ms/step - loss: 6.5296 - lr: 0.0010\n",
            "Epoch 11/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.3219\n",
            "Epoch 11: loss improved from 6.52959 to 6.32187, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 324ms/step - loss: 6.3219 - lr: 0.0010\n",
            "Epoch 12/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.1372\n",
            "Epoch 12: loss improved from 6.32187 to 6.13719, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 339ms/step - loss: 6.1372 - lr: 0.0010\n",
            "Epoch 13/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.9514\n",
            "Epoch 13: loss improved from 6.13719 to 5.95145, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 310ms/step - loss: 5.9514 - lr: 0.0010\n",
            "Epoch 14/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.7828\n",
            "Epoch 14: loss improved from 5.95145 to 5.78285, saving model to nextword1.h5\n",
            "61/61 [==============================] - 26s 429ms/step - loss: 5.7828 - lr: 0.0010\n",
            "Epoch 15/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.6135\n",
            "Epoch 15: loss improved from 5.78285 to 5.61345, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 333ms/step - loss: 5.6135 - lr: 0.0010\n",
            "Epoch 16/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.4589\n",
            "Epoch 16: loss improved from 5.61345 to 5.45891, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 340ms/step - loss: 5.4589 - lr: 0.0010\n",
            "Epoch 17/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.3197\n",
            "Epoch 17: loss improved from 5.45891 to 5.31965, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 338ms/step - loss: 5.3197 - lr: 0.0010\n",
            "Epoch 18/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.1858\n",
            "Epoch 18: loss improved from 5.31965 to 5.18582, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 340ms/step - loss: 5.1858 - lr: 0.0010\n",
            "Epoch 19/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.0621\n",
            "Epoch 19: loss improved from 5.18582 to 5.06206, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 329ms/step - loss: 5.0621 - lr: 0.0010\n",
            "Epoch 20/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.9665\n",
            "Epoch 20: loss improved from 5.06206 to 4.96648, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 344ms/step - loss: 4.9665 - lr: 0.0010\n",
            "Epoch 21/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.8583\n",
            "Epoch 21: loss improved from 4.96648 to 4.85835, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 314ms/step - loss: 4.8583 - lr: 0.0010\n",
            "Epoch 22/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.7344\n",
            "Epoch 22: loss improved from 4.85835 to 4.73444, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 337ms/step - loss: 4.7344 - lr: 0.0010\n",
            "Epoch 23/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.6557\n",
            "Epoch 23: loss improved from 4.73444 to 4.65566, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 309ms/step - loss: 4.6557 - lr: 0.0010\n",
            "Epoch 24/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.5581\n",
            "Epoch 24: loss improved from 4.65566 to 4.55811, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 340ms/step - loss: 4.5581 - lr: 0.0010\n",
            "Epoch 25/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.4911\n",
            "Epoch 25: loss improved from 4.55811 to 4.49111, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 343ms/step - loss: 4.4911 - lr: 0.0010\n",
            "Epoch 26/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3896\n",
            "Epoch 26: loss improved from 4.49111 to 4.38956, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 330ms/step - loss: 4.3896 - lr: 0.0010\n",
            "Epoch 27/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3363\n",
            "Epoch 27: loss improved from 4.38956 to 4.33626, saving model to nextword1.h5\n",
            "61/61 [==============================] - 22s 359ms/step - loss: 4.3363 - lr: 0.0010\n",
            "Epoch 28/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.2692\n",
            "Epoch 28: loss improved from 4.33626 to 4.26921, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 349ms/step - loss: 4.2692 - lr: 0.0010\n",
            "Epoch 29/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.1676\n",
            "Epoch 29: loss improved from 4.26921 to 4.16757, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 329ms/step - loss: 4.1676 - lr: 0.0010\n",
            "Epoch 30/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0873\n",
            "Epoch 30: loss improved from 4.16757 to 4.08730, saving model to nextword1.h5\n",
            "61/61 [==============================] - 22s 365ms/step - loss: 4.0873 - lr: 0.0010\n",
            "Epoch 31/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0482\n",
            "Epoch 31: loss improved from 4.08730 to 4.04816, saving model to nextword1.h5\n",
            "61/61 [==============================] - 22s 354ms/step - loss: 4.0482 - lr: 0.0010\n",
            "Epoch 32/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9415\n",
            "Epoch 32: loss improved from 4.04816 to 3.94153, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 322ms/step - loss: 3.9415 - lr: 0.0010\n",
            "Epoch 33/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8511\n",
            "Epoch 33: loss improved from 3.94153 to 3.85107, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 340ms/step - loss: 3.8511 - lr: 0.0010\n",
            "Epoch 34/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7597\n",
            "Epoch 34: loss improved from 3.85107 to 3.75974, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 317ms/step - loss: 3.7597 - lr: 0.0010\n",
            "Epoch 35/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.6781\n",
            "Epoch 35: loss improved from 3.75974 to 3.67808, saving model to nextword1.h5\n",
            "61/61 [==============================] - 22s 366ms/step - loss: 3.6781 - lr: 0.0010\n",
            "Epoch 36/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.5484\n",
            "Epoch 36: loss improved from 3.67808 to 3.54837, saving model to nextword1.h5\n",
            "61/61 [==============================] - 22s 355ms/step - loss: 3.5484 - lr: 0.0010\n",
            "Epoch 37/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.4632\n",
            "Epoch 37: loss improved from 3.54837 to 3.46322, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 328ms/step - loss: 3.4632 - lr: 0.0010\n",
            "Epoch 38/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.3513\n",
            "Epoch 38: loss improved from 3.46322 to 3.35133, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 345ms/step - loss: 3.3513 - lr: 0.0010\n",
            "Epoch 39/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.2776\n",
            "Epoch 39: loss improved from 3.35133 to 3.27764, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 318ms/step - loss: 3.2776 - lr: 0.0010\n",
            "Epoch 40/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.1849\n",
            "Epoch 40: loss improved from 3.27764 to 3.18490, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 344ms/step - loss: 3.1849 - lr: 0.0010\n",
            "Epoch 41/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.0555\n",
            "Epoch 41: loss improved from 3.18490 to 3.05545, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 315ms/step - loss: 3.0555 - lr: 0.0010\n",
            "Epoch 42/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.0085\n",
            "Epoch 42: loss improved from 3.05545 to 3.00851, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 310ms/step - loss: 3.0085 - lr: 0.0010\n",
            "Epoch 43/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.9530\n",
            "Epoch 43: loss improved from 3.00851 to 2.95300, saving model to nextword1.h5\n",
            "61/61 [==============================] - 22s 363ms/step - loss: 2.9530 - lr: 0.0010\n",
            "Epoch 44/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8861\n",
            "Epoch 44: loss improved from 2.95300 to 2.88606, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 317ms/step - loss: 2.8861 - lr: 0.0010\n",
            "Epoch 45/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8122\n",
            "Epoch 45: loss improved from 2.88606 to 2.81218, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 335ms/step - loss: 2.8122 - lr: 0.0010\n",
            "Epoch 46/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.7207\n",
            "Epoch 46: loss improved from 2.81218 to 2.72066, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 304ms/step - loss: 2.7207 - lr: 0.0010\n",
            "Epoch 47/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.7029\n",
            "Epoch 47: loss improved from 2.72066 to 2.70293, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 330ms/step - loss: 2.7029 - lr: 0.0010\n",
            "Epoch 48/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.6317\n",
            "Epoch 48: loss improved from 2.70293 to 2.63173, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 304ms/step - loss: 2.6317 - lr: 0.0010\n",
            "Epoch 49/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5775\n",
            "Epoch 49: loss improved from 2.63173 to 2.57749, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 327ms/step - loss: 2.5775 - lr: 0.0010\n",
            "Epoch 50/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5160\n",
            "Epoch 50: loss improved from 2.57749 to 2.51597, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 335ms/step - loss: 2.5160 - lr: 0.0010\n",
            "Epoch 51/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4771\n",
            "Epoch 51: loss improved from 2.51597 to 2.47711, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 326ms/step - loss: 2.4771 - lr: 0.0010\n",
            "Epoch 52/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4314\n",
            "Epoch 52: loss improved from 2.47711 to 2.43139, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 313ms/step - loss: 2.4314 - lr: 0.0010\n",
            "Epoch 53/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3931\n",
            "Epoch 53: loss improved from 2.43139 to 2.39311, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 329ms/step - loss: 2.3931 - lr: 0.0010\n",
            "Epoch 54/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3319\n",
            "Epoch 54: loss improved from 2.39311 to 2.33185, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 309ms/step - loss: 2.3319 - lr: 0.0010\n",
            "Epoch 55/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2792\n",
            "Epoch 55: loss improved from 2.33185 to 2.27917, saving model to nextword1.h5\n",
            "61/61 [==============================] - 22s 361ms/step - loss: 2.2792 - lr: 0.0010\n",
            "Epoch 56/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2792\n",
            "Epoch 56: loss improved from 2.27917 to 2.27917, saving model to nextword1.h5\n",
            "61/61 [==============================] - 23s 380ms/step - loss: 2.2792 - lr: 0.0010\n",
            "Epoch 57/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2556\n",
            "Epoch 57: loss improved from 2.27917 to 2.25556, saving model to nextword1.h5\n",
            "61/61 [==============================] - 23s 378ms/step - loss: 2.2556 - lr: 0.0010\n",
            "Epoch 58/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1865\n",
            "Epoch 58: loss improved from 2.25556 to 2.18651, saving model to nextword1.h5\n",
            "61/61 [==============================] - 23s 371ms/step - loss: 2.1865 - lr: 0.0010\n",
            "Epoch 59/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1052\n",
            "Epoch 59: loss improved from 2.18651 to 2.10520, saving model to nextword1.h5\n",
            "61/61 [==============================] - 23s 367ms/step - loss: 2.1052 - lr: 0.0010\n",
            "Epoch 60/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0743\n",
            "Epoch 60: loss improved from 2.10520 to 2.07434, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 338ms/step - loss: 2.0743 - lr: 0.0010\n",
            "Epoch 61/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0563\n",
            "Epoch 61: loss improved from 2.07434 to 2.05629, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 302ms/step - loss: 2.0563 - lr: 0.0010\n",
            "Epoch 62/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0381\n",
            "Epoch 62: loss improved from 2.05629 to 2.03807, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 321ms/step - loss: 2.0381 - lr: 0.0010\n",
            "Epoch 63/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0235\n",
            "Epoch 63: loss improved from 2.03807 to 2.02346, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 302ms/step - loss: 2.0235 - lr: 0.0010\n",
            "Epoch 64/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0149\n",
            "Epoch 64: loss improved from 2.02346 to 2.01494, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 338ms/step - loss: 2.0149 - lr: 0.0010\n",
            "Epoch 65/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9653\n",
            "Epoch 65: loss improved from 2.01494 to 1.96531, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 304ms/step - loss: 1.9653 - lr: 0.0010\n",
            "Epoch 66/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9386\n",
            "Epoch 66: loss improved from 1.96531 to 1.93858, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 329ms/step - loss: 1.9386 - lr: 0.0010\n",
            "Epoch 67/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9114\n",
            "Epoch 67: loss improved from 1.93858 to 1.91136, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 306ms/step - loss: 1.9114 - lr: 0.0010\n",
            "Epoch 68/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8885\n",
            "Epoch 68: loss improved from 1.91136 to 1.88850, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 340ms/step - loss: 1.8885 - lr: 0.0010\n",
            "Epoch 69/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8802\n",
            "Epoch 69: loss improved from 1.88850 to 1.88025, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 296ms/step - loss: 1.8802 - lr: 0.0010\n",
            "Epoch 70/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8371\n",
            "Epoch 70: loss improved from 1.88025 to 1.83706, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 334ms/step - loss: 1.8371 - lr: 0.0010\n",
            "Epoch 71/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8138\n",
            "Epoch 71: loss improved from 1.83706 to 1.81375, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 307ms/step - loss: 1.8138 - lr: 0.0010\n",
            "Epoch 72/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8008\n",
            "Epoch 72: loss improved from 1.81375 to 1.80079, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 317ms/step - loss: 1.8008 - lr: 0.0010\n",
            "Epoch 73/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7940\n",
            "Epoch 73: loss improved from 1.80079 to 1.79396, saving model to nextword1.h5\n",
            "61/61 [==============================] - 25s 407ms/step - loss: 1.7940 - lr: 0.0010\n",
            "Epoch 74/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7773\n",
            "Epoch 74: loss improved from 1.79396 to 1.77727, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 328ms/step - loss: 1.7773 - lr: 0.0010\n",
            "Epoch 75/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7353\n",
            "Epoch 75: loss improved from 1.77727 to 1.73527, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 327ms/step - loss: 1.7353 - lr: 0.0010\n",
            "Epoch 76/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7043\n",
            "Epoch 76: loss improved from 1.73527 to 1.70432, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 309ms/step - loss: 1.7043 - lr: 0.0010\n",
            "Epoch 77/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6928\n",
            "Epoch 77: loss improved from 1.70432 to 1.69276, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 334ms/step - loss: 1.6928 - lr: 0.0010\n",
            "Epoch 78/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6971\n",
            "Epoch 78: loss did not improve from 1.69276\n",
            "61/61 [==============================] - 19s 305ms/step - loss: 1.6971 - lr: 0.0010\n",
            "Epoch 79/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6758\n",
            "Epoch 79: loss improved from 1.69276 to 1.67576, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 321ms/step - loss: 1.6758 - lr: 0.0010\n",
            "Epoch 80/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6653\n",
            "Epoch 80: loss improved from 1.67576 to 1.66533, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 302ms/step - loss: 1.6653 - lr: 0.0010\n",
            "Epoch 81/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6682\n",
            "Epoch 81: loss did not improve from 1.66533\n",
            "61/61 [==============================] - 19s 315ms/step - loss: 1.6682 - lr: 0.0010\n",
            "Epoch 82/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6465\n",
            "Epoch 82: loss improved from 1.66533 to 1.64652, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 303ms/step - loss: 1.6465 - lr: 0.0010\n",
            "Epoch 83/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6065\n",
            "Epoch 83: loss improved from 1.64652 to 1.60646, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 324ms/step - loss: 1.6065 - lr: 0.0010\n",
            "Epoch 84/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5961\n",
            "Epoch 84: loss improved from 1.60646 to 1.59605, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 302ms/step - loss: 1.5961 - lr: 0.0010\n",
            "Epoch 85/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5910\n",
            "Epoch 85: loss improved from 1.59605 to 1.59104, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 332ms/step - loss: 1.5910 - lr: 0.0010\n",
            "Epoch 86/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5771\n",
            "Epoch 86: loss improved from 1.59104 to 1.57706, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 306ms/step - loss: 1.5771 - lr: 0.0010\n",
            "Epoch 87/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5760\n",
            "Epoch 87: loss improved from 1.57706 to 1.57604, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 340ms/step - loss: 1.5760 - lr: 0.0010\n",
            "Epoch 88/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5587\n",
            "Epoch 88: loss improved from 1.57604 to 1.55870, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 303ms/step - loss: 1.5587 - lr: 0.0010\n",
            "Epoch 89/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5602\n",
            "Epoch 89: loss did not improve from 1.55870\n",
            "61/61 [==============================] - 20s 331ms/step - loss: 1.5602 - lr: 0.0010\n",
            "Epoch 90/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5756\n",
            "Epoch 90: loss did not improve from 1.55870\n",
            "61/61 [==============================] - 20s 321ms/step - loss: 1.5756 - lr: 0.0010\n",
            "Epoch 91/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5593\n",
            "Epoch 91: loss did not improve from 1.55870\n",
            "\n",
            "Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "61/61 [==============================] - 19s 314ms/step - loss: 1.5593 - lr: 0.0010\n",
            "Epoch 92/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1644\n",
            "Epoch 92: loss improved from 1.55870 to 1.16435, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 310ms/step - loss: 1.1644 - lr: 2.0000e-04\n",
            "Epoch 93/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0060\n",
            "Epoch 93: loss improved from 1.16435 to 1.00600, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 322ms/step - loss: 1.0060 - lr: 2.0000e-04\n",
            "Epoch 94/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9504\n",
            "Epoch 94: loss improved from 1.00600 to 0.95040, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 302ms/step - loss: 0.9504 - lr: 2.0000e-04\n",
            "Epoch 95/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9195\n",
            "Epoch 95: loss improved from 0.95040 to 0.91946, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 320ms/step - loss: 0.9195 - lr: 2.0000e-04\n",
            "Epoch 96/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8988\n",
            "Epoch 96: loss improved from 0.91946 to 0.89876, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 302ms/step - loss: 0.8988 - lr: 2.0000e-04\n",
            "Epoch 97/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8848\n",
            "Epoch 97: loss improved from 0.89876 to 0.88484, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 336ms/step - loss: 0.8848 - lr: 2.0000e-04\n",
            "Epoch 98/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8756\n",
            "Epoch 98: loss improved from 0.88484 to 0.87564, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 307ms/step - loss: 0.8756 - lr: 2.0000e-04\n",
            "Epoch 99/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8671\n",
            "Epoch 99: loss improved from 0.87564 to 0.86705, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 318ms/step - loss: 0.8671 - lr: 2.0000e-04\n",
            "Epoch 100/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8585\n",
            "Epoch 100: loss improved from 0.86705 to 0.85850, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 309ms/step - loss: 0.8585 - lr: 2.0000e-04\n",
            "Epoch 101/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8547\n",
            "Epoch 101: loss improved from 0.85850 to 0.85470, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 318ms/step - loss: 0.8547 - lr: 2.0000e-04\n",
            "Epoch 102/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8488\n",
            "Epoch 102: loss improved from 0.85470 to 0.84877, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 307ms/step - loss: 0.8488 - lr: 2.0000e-04\n",
            "Epoch 103/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8449\n",
            "Epoch 103: loss improved from 0.84877 to 0.84488, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 331ms/step - loss: 0.8449 - lr: 2.0000e-04\n",
            "Epoch 104/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8361\n",
            "Epoch 104: loss improved from 0.84488 to 0.83610, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 305ms/step - loss: 0.8361 - lr: 2.0000e-04\n",
            "Epoch 105/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8352\n",
            "Epoch 105: loss improved from 0.83610 to 0.83521, saving model to nextword1.h5\n",
            "61/61 [==============================] - 25s 418ms/step - loss: 0.8352 - lr: 2.0000e-04\n",
            "Epoch 106/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8297\n",
            "Epoch 106: loss improved from 0.83521 to 0.82972, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 337ms/step - loss: 0.8297 - lr: 2.0000e-04\n",
            "Epoch 107/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8275\n",
            "Epoch 107: loss improved from 0.82972 to 0.82755, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 303ms/step - loss: 0.8275 - lr: 2.0000e-04\n",
            "Epoch 108/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8230\n",
            "Epoch 108: loss improved from 0.82755 to 0.82301, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 313ms/step - loss: 0.8230 - lr: 2.0000e-04\n",
            "Epoch 109/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8231\n",
            "Epoch 109: loss did not improve from 0.82301\n",
            "61/61 [==============================] - 18s 299ms/step - loss: 0.8231 - lr: 2.0000e-04\n",
            "Epoch 110/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8190\n",
            "Epoch 110: loss improved from 0.82301 to 0.81898, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 322ms/step - loss: 0.8190 - lr: 2.0000e-04\n",
            "Epoch 111/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8198\n",
            "Epoch 111: loss did not improve from 0.81898\n",
            "61/61 [==============================] - 18s 297ms/step - loss: 0.8198 - lr: 2.0000e-04\n",
            "Epoch 112/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8181\n",
            "Epoch 112: loss improved from 0.81898 to 0.81814, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 321ms/step - loss: 0.8181 - lr: 2.0000e-04\n",
            "Epoch 113/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8116\n",
            "Epoch 113: loss improved from 0.81814 to 0.81156, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 301ms/step - loss: 0.8116 - lr: 2.0000e-04\n",
            "Epoch 114/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8069\n",
            "Epoch 114: loss improved from 0.81156 to 0.80692, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 316ms/step - loss: 0.8069 - lr: 2.0000e-04\n",
            "Epoch 115/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8078\n",
            "Epoch 115: loss did not improve from 0.80692\n",
            "61/61 [==============================] - 19s 317ms/step - loss: 0.8078 - lr: 2.0000e-04\n",
            "Epoch 116/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8060\n",
            "Epoch 116: loss improved from 0.80692 to 0.80602, saving model to nextword1.h5\n",
            "61/61 [==============================] - 22s 367ms/step - loss: 0.8060 - lr: 2.0000e-04\n",
            "Epoch 117/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8009\n",
            "Epoch 117: loss improved from 0.80602 to 0.80093, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 309ms/step - loss: 0.8009 - lr: 2.0000e-04\n",
            "Epoch 118/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7999\n",
            "Epoch 118: loss improved from 0.80093 to 0.79990, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 326ms/step - loss: 0.7999 - lr: 2.0000e-04\n",
            "Epoch 119/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7958\n",
            "Epoch 119: loss improved from 0.79990 to 0.79575, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 312ms/step - loss: 0.7958 - lr: 2.0000e-04\n",
            "Epoch 120/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7999\n",
            "Epoch 120: loss did not improve from 0.79575\n",
            "61/61 [==============================] - 21s 336ms/step - loss: 0.7999 - lr: 2.0000e-04\n",
            "Epoch 121/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7947\n",
            "Epoch 121: loss improved from 0.79575 to 0.79470, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 305ms/step - loss: 0.7947 - lr: 2.0000e-04\n",
            "Epoch 122/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7937\n",
            "Epoch 122: loss improved from 0.79470 to 0.79367, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 317ms/step - loss: 0.7937 - lr: 2.0000e-04\n",
            "Epoch 123/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7915\n",
            "Epoch 123: loss improved from 0.79367 to 0.79154, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 313ms/step - loss: 0.7915 - lr: 2.0000e-04\n",
            "Epoch 124/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7870\n",
            "Epoch 124: loss improved from 0.79154 to 0.78696, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 334ms/step - loss: 0.7870 - lr: 2.0000e-04\n",
            "Epoch 125/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7870\n",
            "Epoch 125: loss did not improve from 0.78696\n",
            "61/61 [==============================] - 21s 350ms/step - loss: 0.7870 - lr: 2.0000e-04\n",
            "Epoch 126/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7839\n",
            "Epoch 126: loss improved from 0.78696 to 0.78388, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 311ms/step - loss: 0.7839 - lr: 2.0000e-04\n",
            "Epoch 127/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7862\n",
            "Epoch 127: loss did not improve from 0.78388\n",
            "61/61 [==============================] - 20s 328ms/step - loss: 0.7862 - lr: 2.0000e-04\n",
            "Epoch 128/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7858\n",
            "Epoch 128: loss did not improve from 0.78388\n",
            "61/61 [==============================] - 18s 301ms/step - loss: 0.7858 - lr: 2.0000e-04\n",
            "Epoch 129/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7817\n",
            "Epoch 129: loss improved from 0.78388 to 0.78175, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 330ms/step - loss: 0.7817 - lr: 2.0000e-04\n",
            "Epoch 130/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7787\n",
            "Epoch 130: loss improved from 0.78175 to 0.77866, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 325ms/step - loss: 0.7787 - lr: 2.0000e-04\n",
            "Epoch 131/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7791\n",
            "Epoch 131: loss did not improve from 0.77866\n",
            "61/61 [==============================] - 20s 319ms/step - loss: 0.7791 - lr: 2.0000e-04\n",
            "Epoch 132/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7771\n",
            "Epoch 132: loss improved from 0.77866 to 0.77714, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 325ms/step - loss: 0.7771 - lr: 2.0000e-04\n",
            "Epoch 133/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7776\n",
            "Epoch 133: loss did not improve from 0.77714\n",
            "61/61 [==============================] - 19s 301ms/step - loss: 0.7776 - lr: 2.0000e-04\n",
            "Epoch 134/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7736\n",
            "Epoch 134: loss improved from 0.77714 to 0.77356, saving model to nextword1.h5\n",
            "61/61 [==============================] - 21s 338ms/step - loss: 0.7736 - lr: 2.0000e-04\n",
            "Epoch 135/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7740\n",
            "Epoch 135: loss did not improve from 0.77356\n",
            "61/61 [==============================] - 20s 318ms/step - loss: 0.7740 - lr: 2.0000e-04\n",
            "Epoch 136/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7707\n",
            "Epoch 136: loss improved from 0.77356 to 0.77069, saving model to nextword1.h5\n",
            "61/61 [==============================] - 25s 414ms/step - loss: 0.7707 - lr: 2.0000e-04\n",
            "Epoch 137/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7683\n",
            "Epoch 137: loss improved from 0.77069 to 0.76830, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 309ms/step - loss: 0.7683 - lr: 2.0000e-04\n",
            "Epoch 138/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7693\n",
            "Epoch 138: loss did not improve from 0.76830\n",
            "61/61 [==============================] - 20s 320ms/step - loss: 0.7693 - lr: 2.0000e-04\n",
            "Epoch 139/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7675\n",
            "Epoch 139: loss improved from 0.76830 to 0.76753, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 323ms/step - loss: 0.7675 - lr: 2.0000e-04\n",
            "Epoch 140/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7681\n",
            "Epoch 140: loss did not improve from 0.76753\n",
            "61/61 [==============================] - 20s 322ms/step - loss: 0.7681 - lr: 2.0000e-04\n",
            "Epoch 141/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7673\n",
            "Epoch 141: loss improved from 0.76753 to 0.76731, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 308ms/step - loss: 0.7673 - lr: 2.0000e-04\n",
            "Epoch 142/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7647\n",
            "Epoch 142: loss improved from 0.76731 to 0.76474, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 323ms/step - loss: 0.7647 - lr: 2.0000e-04\n",
            "Epoch 143/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7635\n",
            "Epoch 143: loss improved from 0.76474 to 0.76352, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 334ms/step - loss: 0.7635 - lr: 2.0000e-04\n",
            "Epoch 144/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7614\n",
            "Epoch 144: loss improved from 0.76352 to 0.76143, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 313ms/step - loss: 0.7614 - lr: 2.0000e-04\n",
            "Epoch 145/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7619\n",
            "Epoch 145: loss did not improve from 0.76143\n",
            "61/61 [==============================] - 18s 296ms/step - loss: 0.7619 - lr: 2.0000e-04\n",
            "Epoch 146/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7612\n",
            "Epoch 146: loss improved from 0.76143 to 0.76119, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 321ms/step - loss: 0.7612 - lr: 2.0000e-04\n",
            "Epoch 147/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7604\n",
            "Epoch 147: loss improved from 0.76119 to 0.76037, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 298ms/step - loss: 0.7604 - lr: 2.0000e-04\n",
            "Epoch 148/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7610\n",
            "Epoch 148: loss did not improve from 0.76037\n",
            "61/61 [==============================] - 19s 311ms/step - loss: 0.7610 - lr: 2.0000e-04\n",
            "Epoch 149/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7559\n",
            "Epoch 149: loss improved from 0.76037 to 0.75594, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 310ms/step - loss: 0.7559 - lr: 2.0000e-04\n",
            "Epoch 150/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7589\n",
            "Epoch 150: loss did not improve from 0.75594\n",
            "61/61 [==============================] - 20s 333ms/step - loss: 0.7589 - lr: 2.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f20996d3df0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\n",
        "\n",
        "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXe_Yme0hkJ_"
      },
      "source": [
        "### Creating the Prediction Script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hV8HUCsshkKF"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "model = load_model('/content/nextword1.h5')\n",
        "tokenizer = pickle.load(open('/content/tokenizer1.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "    for i in range(3):\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = np.array(sequence)\n",
        "        \n",
        "        preds = np.argmax(model.predict(sequence), axis=1)\n",
        "        predicted_word = \"\"\n",
        "        \n",
        "        for key, value in tokenizer.word_index.items():\n",
        "            if value == preds:\n",
        "                predicted_word = key\n",
        "                break\n",
        "        \n",
        "        print(predicted_word)\n",
        "        return predicted_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qA_wmCRMhkKH",
        "outputId": "ce8d035c-4d50-4d1a-8480-cbb1e165c233",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your line: floundering\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "only\n",
            "Enter your line: he was used to\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "cover\n",
            "Enter your line: strenuous\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "career\n",
            "Enter your line: irregular\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "food\n",
            "Enter your line: stop the script\n",
            "Ending The Program.....\n"
          ]
        }
      ],
      "source": [
        "while(True):\n",
        "\n",
        "    text = input(\"Enter your line: \")\n",
        "    \n",
        "    if text == \"stop the script\":\n",
        "        print(\"Ending The Program.....\")\n",
        "        break\n",
        "    \n",
        "    else:\n",
        "        text = text.split(\" \")\n",
        "        text = text[-1]\n",
        "\n",
        "        text = ''.join(text)\n",
        "        Predict_Next_Words(model, tokenizer, text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}